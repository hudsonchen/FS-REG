#!/bin/bash

# NOTE: Lines starting with "#SBATCH" are valid SLURM commands or statements,
#       while those starting with "#" and "##SBATCH" are comments.  Uncomment
#       "##SBATCH" line means to remove one # and start with #SBATCH to be a
#       SLURM command or statement.


#SBATCH -J slurm_job #Slurm job name

# Set the maximum runtime, uncomment if you need it
##SBATCH -t 48:00:00 #Maximum runtime of 48 hours

# Enable email notificaitons when job begins and ends, uncomment if you need it
##SBATCH --mail-user=user_name@ust.hk #Update your email address
##SBATCH --mail-type=begin
##SBATCH --mail-type=end

# Choose partition (queue) "gpu" or "gpu-share"
#SBATCH -p gpu

# To use 4 cpu cores and 2 gpu devices in a node
#SBATCH -N 1 -n 8 --gres=gpu:8

# Setup runtime environment if necessary
# or you can source ~/.bashrc or ~/.bash_profile

# export XLA_PYTHON_CLIENT_PREALLOCATE="false"
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-10.2:/usr/local/cuda-10.2/lib64:/usr/local/cuda-10.2/include
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda:/usr/local/cuda/lib64:/usr/local/cuda/include

# Go to the job submission directory and run your application
cd $HOME/apps/slurm
# Execute applications in parallel
srun -p gpu -n 1 --gres=gpu:8 --cpus-per-task=8 /home/miniconda3/envs/fmap/bin/python3.8 /home/function_map/mlp_mixer/classification_mixer.py --lr 0.01 --dataset cifar100 --method map --optimizer sgd --reg 0.00 --inverse --dummy_input_dim 2 --element_wise --epochs 30 --train_size 50000 --batch_size 128 --aug --seed 2 --save --save_path /home/function_map/results
wait
